"""
ENTERPRISE CONTENT TAGGER SYSTEM
Developed by: [SiinNoBox Team]
Version: 14.1 (UI Fixed)
Description: Automated image analysis and metadata tagging tool using OpenCLIP.
"""

import streamlit as st
import pandas as pd
from PIL import Image
import io
import torch
import open_clip
import logging
from typing import List, Dict, Optional

# --- 1. SYSTEM CONFIGURATION ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - [%(levelname)s] - %(message)s')
logger = logging.getLogger(__name__)

CONFIG = {
    "MAX_IMAGES": 100,
    "MAX_FILE_SIZE_MB": 10,
    "THUMBNAIL_SIZE": (300, 600),
    "CLIP_INPUT_SIZE": (224, 224),
    "MODEL_NAME": "ViT-B-32",
    "PRETRAINED": "openai"
}

# --- 2. UI/UX CONFIGURATION ---
st.set_page_config(
    page_title="Enterprise Content Tagger",
    page_icon="üíº",
    layout="wide",
    initial_sidebar_state="expanded" # M·∫∑c ƒë·ªãnh m·ªü, nh∆∞ng v·∫´n c√≥ n√∫t ƒë√≥ng
)

# Custom CSS: Giao di·ªán ph·∫≥ng, chuy√™n nghi·ªáp + FIX L·ªñI M·∫§T N√öT SIDEBAR
st.markdown("""
    <style>
    /* T·ªïng th·ªÉ container */
    .main { background-color: #ffffff; }
    
    /* Card s·∫£n ph·∫©m */
    div[data-testid="stVerticalBlock"] > div[data-testid="stVerticalBlock"] {
        background-color: #f8f9fa;
        padding: 15px;
        border-radius: 6px;
        border: 1px solid #e9ecef;
    }
    
    /* H√¨nh ·∫£nh */
    div[data-testid="stImage"] img { border-radius: 4px; object-fit: contain; }
    
    /* N√∫t b·∫•m Primary */
    div[data-testid="stButton"] > button[kind="primary"] {
        background-color: #0f5132 !important;
        border-color: #0f5132 !important;
        color: white !important;
        border-radius: 4px;
        font-weight: 500;
        text-transform: uppercase;
        letter-spacing: 0.5px;
    }
    
    /* N√∫t Download */
    div[data-testid="stDownloadButton"] > button {
        background-color: #0f5132 !important;
        border-color: #0f5132 !important;
        color: white !important;
        border-radius: 4px;
        font-weight: 500;
        width: 100%;
    }

    /* Typography */
    h1, h2, h3 { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; color: #212529; }
    .stSelectbox, .stTextInput { font-size: 0.9rem; }
    div[data-testid="stCaptionContainer"] { font-size: 0.8rem; color: #6c757d; }

    /* --- [FIX] KH√îI PH·ª§C N√öT ƒê√ìNG/M·ªû SIDEBAR --- */
    
    /* 1. Hi·ªÉn th·ªã r√µ r√†ng n√∫t m≈©i t√™n (Chevron) ·ªü g√≥c tr√°i */
    button[kind="header"] {
        background-color: transparent !important;
        color: #212529 !important; /* M√†u ƒëen ƒë·∫≠m ƒë·ªÉ d·ªÖ nh√¨n */
        opacity: 1 !important;
        display: block !important;
        z-index: 999999 !important; /* Lu√¥n n·∫±m tr√™n c√πng */
    }
    
    /* 2. ƒê·∫£m b·∫£o thanh header c·ªßa sidebar kh√¥ng b·ªã ·∫©n */
    div[data-testid="stSidebarNav"] {
        display: block !important;
    }

    /* 3. M√†u s·∫Øc khi hover v√†o n√∫t ƒë√≥ng m·ªü */
    button[kind="header"]:hover {
        color: #0f5132 !important; /* Xanh doanh nghi·ªáp khi di chu·ªôt */
        background-color: #f0f0f0 !important;
    }
    </style>
""", unsafe_allow_html=True)

st.title("H·ªÜ TH·ªêNG PH√ÇN T√çCH & T·ªêI ∆ØU H√ìA N·ªòI DUNG")
st.markdown("**Phi√™n b·∫£n Doanh nghi·ªáp (Enterprise Edition)** | Powered by OpenCLIP AI")
st.divider()

# --- 3. DATA DICTIONARIES ---
AI_STYLES = [
    "2D", "3D", "Cute", "Animeart", "Realism",
    "Aesthetic", "Cool", "Fantasy", "Comic", "Horror",
    "Cyberpunk", "Lofi", "Minimalism", "Digitalart", "Cinematic",
    "Pixelart", "Scifi", "Vangoghart"
]
AI_COLORS = [
    "Black", "White", "Blackandwhite", "Red", "Yellow",
    "Blue", "Green", "Pink", "Orange", "Pastel",
    "Hologram", "Vintage", "Colorful", "Neutral", "Light",
    "Dark", "Warm", "Cold", "Neon", "Gradient",
    "Purple", "Brown", "Grey"
]
UI_STYLES = ["None"] + AI_STYLES
UI_COLORS = ["None"] + AI_COLORS
UI_MOODS = ["None", "Happy", "Sad", "Lonely", "Lovely", "Funny", "ZenMode"]
UI_GENDERS = ["None", "Male", "Female", "Non-binary", "Unisex"]

# Guardrails Logic
STYLE_PROMPT_MAP = {
    "2D": "flat 2d illustration vector art cartoon style",
    "3D": "3d computer graphics blender render c4d realistic material",
    "Cute": "cute kawaii chibi adorable character design soft shapes",
    "Animeart": "anime style japanese manga illustration cel shaded",
    "Realism": "photorealistic photography 4k high definition real life",
    "Aesthetic": "aesthetic artistic beautiful composition trending on artstation",
    "Cool": "cool stylish edgy fashion streetwear vibe",
    "Fantasy": "fantasy art magic dungeons and dragons medieval warrior",
    "Comic": "comic book style bold lines pop art western comic marvel dc",
    "Horror": "horror scary creepy dark nightmare monster gore",
    "Cyberpunk": "cyberpunk futuristic sci-fi neon high tech city low life",
    "Lofi": "lofi hip hop style chill retro anime aesthetic study girl",
    "Minimalism": "minimalism simple clean lines minimal art negative space",
    "Digitalart": "digital art digital painting wacom tablet drawing concept art",
    "Cinematic": "cinematic movie scene dramatic lighting wide shot film grain",
    "Pixelart": "pixel art 8-bit retro video game style sprite",
    "Scifi": "sci-fi science fiction space future technology alien spaceship",
    "Vangoghart": "vincent van gogh style starry night impressionism oil painting swirl"
}
COLOR_PROMPT_MAP = {
    "Black": "mostly black dark void background",
    "White": "mostly pure white bright background",
    "Blackandwhite": "black and white monochrome photography greyscale",
    "Red": "dominant bright red color object or clothes",
    "Yellow": "dominant bright yellow color sunlight or object",
    "Blue": "dominant blue color sky ocean or object",
    "Green": "dominant green color nature plants or object",
    "Pink": "dominant pink color cute flower or object",
    "Orange": "dominant orange color sunset or fruit",
    "Pastel": "soft pastel colors light desaturated tones",
    "Hologram": "holographic iridescent rainbow silver metallic texture",
    "Vintage": "vintage retro style sepia old photo paper",
    "Colorful": "many different vibrant colors rainbow confetti",
    "Neutral": "neutral beige earth tones minimalist skin tone",
    "Light": "bright high key lighting sunny atmosphere",
    "Dark": "dark dim lighting low light night shadow",
    "Warm": "warm colors temperature red orange yellow heating",
    "Cold": "cold colors temperature blue cyan ice cool lighting",
    "Neon": "glowing neon lights cyberpunk laser",
    "Gradient": "smooth color gradient transition blurred background",
    "Purple": "dominant purple violet lavender color",
    "Brown": "dominant brown color wood earth chocolate",
    "Grey": "dominant grey color concrete silver metal",
}

# --- 4. CORE ENGINE ---
@st.cache_resource
def load_ai_engine():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"AI Engine initializing on: {device}")
    try:
        model, _, preprocess = open_clip.create_model_and_transforms(CONFIG["MODEL_NAME"], pretrained=CONFIG["PRETRAINED"], device=device)
        tokenizer = open_clip.get_tokenizer(CONFIG["MODEL_NAME"])
        
        s_texts = [STYLE_PROMPT_MAP.get(s, f"a {s} style artwork") for s in AI_STYLES]
        c_texts = [COLOR_PROMPT_MAP.get(c, f"dominant color is {c}") for c in AI_COLORS]
        
        s_vectors = tokenizer(s_texts).to(device)
        c_vectors = tokenizer(c_texts).to(device)
        
        with torch.no_grad():
            s_feat = model.encode_text(s_vectors)
            c_feat = model.encode_text(c_vectors)
            s_feat /= s_feat.norm(dim=-1, keepdim=True)
            c_feat /= c_feat.norm(dim=-1, keepdim=True)
        return model, preprocess, s_feat, c_feat, device
    except Exception as e:
        logger.error(f"Critical Error: {e}")
        raise e

try:
    with st.spinner("ƒêang kh·ªüi t·∫°o h·ªá th·ªëng x·ª≠ l√Ω AI... Vui l√≤ng ƒë·ª£i."):
        model, preprocess, s_feat, c_feat, device = load_ai_engine()
except Exception as e:
    st.error(f"L·ªói kh·ªüi t·∫°o: {e}"); st.stop()

def analyze_image(file_obj) -> Dict:
    try:
        file_bytes = file_obj.getvalue()
        original_img = Image.open(io.BytesIO(file_bytes))
        if original_img.mode != "RGB": original_img = original_img.convert("RGB")
        
        thumb = original_img.copy()
        thumb.thumbnail(CONFIG["THUMBNAIL_SIZE"])
        
        input_img = preprocess(original_img).unsqueeze(0).to(device)
        with torch.no_grad():
            img_feat = model.encode_image(input_img)
            img_feat /= img_feat.norm(dim=-1, keepdim=True)
            
        s_idx = (100.0 * img_feat @ s_feat.T).softmax(dim=-1).argmax().item()
        c_idx = (100.0 * img_feat @ c_feat.T).softmax(dim=-1).argmax().item()
        
        return {"status": "success", "filename": file_obj.name, "image_obj": thumb, "object": "", 
                "style": AI_STYLES[s_idx], "color": AI_COLORS[c_idx], "mood": "None", "gender": "None"}
    except Exception as e:
        return {"status": "error", "filename": file_obj.name, "msg": str(e)}

# --- 5. UI COMPONENTS ---
def render_image_card(idx: int, item: Dict, start_num: int):
    with st.container(border=True):
        st.image(item["image_obj"], use_container_width=True)
        st.caption(f"STT: {start_num + idx} | File: {item['filename']}")
        
        new_obj = st.text_input("ƒê·ªëi t∆∞·ª£ng (Object)", value=item["object"], key=f"obj_{idx}", label_visibility="collapsed", placeholder="Nh·∫≠p t√™n ƒë·ªëi t∆∞·ª£ng...")
        
        c1, c2 = st.columns(2)
        with c1:
            curr_s = item["style"] if item["style"] in UI_STYLES else "None"
            new_s = st.selectbox("Style", UI_STYLES, index=UI_STYLES.index(curr_s), key=f"s_{idx}", label_visibility="collapsed")
            curr_m = item["mood"] if item["mood"] in UI_MOODS else "None"
            new_m = st.selectbox("Mood", UI_MOODS, index=UI_MOODS.index(curr_m), key=f"m_{idx}", label_visibility="collapsed")
        with c2:
            curr_c = item["color"] if item["color"] in UI_COLORS else "None"
            new_c = st.selectbox("Color", UI_COLORS, index=UI_COLORS.index(curr_c), key=f"c_{idx}", label_visibility="collapsed")
            curr_g = item["gender"] if item["gender"] in UI_GENDERS else "None"
            new_g = st.selectbox("Gender", UI_GENDERS, index=UI_GENDERS.index(curr_g), key=f"g_{idx}", label_visibility="collapsed")

        st.session_state["results"][idx].update({"object": new_obj, "style": new_s, "color": new_c, "mood": new_m, "gender": new_g})

# --- 6. SIDEBAR & MAIN LOGIC ---
with st.sidebar:
    st.header("C·∫•u h√¨nh & D·ªØ li·ªáu")
    st.subheader("C·∫•u h√¨nh hi·ªÉn th·ªã")
    cols_per_row = st.slider("S·ªë c·ªôt:", 2, 6, 4)
    st.divider()
    st.subheader("Nh·∫≠p li·ªáu")
    start_idx = st.number_input("S·ªë th·ª© t·ª± b·∫Øt ƒë·∫ßu:", value=1, step=1)
    uploaded_files = st.file_uploader(f"T·∫£i ·∫£nh l√™n ({CONFIG['MAX_IMAGES']} max):", type=['png','jpg','jpeg','webp'], accept_multiple_files=True)
    st.markdown("---")
    process_btn = st.button("‚ñ∂ X·ª¨ L√ù D·ªÆ LI·ªÜU", type="primary")
    if st.button("‚ü≤ ƒê·∫∑t l·∫°i h·ªá th·ªëng"): st.session_state.clear(); st.rerun()

if "results" not in st.session_state: st.session_state["results"] = []

if process_btn and uploaded_files:
    if len(uploaded_files) > CONFIG["MAX_IMAGES"]: st.error("Qu√° gi·ªõi h·∫°n ·∫£nh."); st.stop()
    processed_results = []
    progress_bar = st.progress(0); status_text = st.empty()
    for i, file in enumerate(uploaded_files):
        status_text.text(f"ƒêang x·ª≠ l√Ω: {file.name}...")
        res = analyze_image(file)
        if res["status"] == "success": res["id"] = i; processed_results.append(res)
        progress_bar.progress((i+1)/len(uploaded_files))
    st.session_state["results"] = processed_results
    status_text.success("X·ª≠ l√Ω ho√†n t·∫•t."); progress_bar.empty()

# --- 7. EXPORT ---
if st.session_state["results"]:
    export_container = st.container(); st.divider()
    grid = st.columns(cols_per_row)
    for i, item in enumerate(st.session_state["results"]):
        with grid[i % cols_per_row]: render_image_card(i, item, start_idx)
            
    with export_container:
        c1, c2 = st.columns([3, 1])
        with c1: st.subheader(f"K·∫øt qu·∫£ ph√¢n t√≠ch ({len(st.session_state['results'])} m·ª•c)")
        with c2:
            export_data = []
            for item in st.session_state["results"]:
                tags = [t for t in [item["object"].strip(), item["style"], item["color"], item["mood"], item["gender"]] if t and t != "None"]
                export_data.append({
                    "STT": start_idx + st.session_state["results"].index(item),
                    "T√™n t·∫≠p tin": item["filename"], "Hashtag T·ªïng h·ª£p": ", ".join(tags),
                    "Object": item["object"], "Style": item["style"], "Color": item["color"], "Mood": item["mood"], "Gender": item["gender"]
                })
            df = pd.DataFrame(export_data)
            buffer = io.BytesIO()
            with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
                df.to_excel(writer, index=False, sheet_name='Data')
                worksheet = writer.sheets['Data']
                worksheet.set_column('A:A', 5); worksheet.set_column('B:B', 25); worksheet.set_column('C:C', 50)
            st.download_button("üì• XU·∫§T B√ÅO C√ÅO EXCEL", buffer.getvalue(), "Analysed_Report.xlsx", "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
elif not uploaded_files: st.info("H·ªá th·ªëng s·∫µn s√†ng. Vui l√≤ng t·∫£i d·ªØ li·ªáu.")
